[Data]
data_dir = corpus
train_data_path = %(data_dir)s/train_data_8.5w.txt
test_data_path = %(data_dir)s/test_data_5k.txt
dev_data_path = %(data_dir)s/dev_data_1k.txt
lexicon_path = %(data_dir)s/emo_words_new.txt
;embedding_path = model/word2vec.model
word2vec_path = %(data_dir)s/sgns.weibo.char
char2vec_path = %(data_dir)s/char_vectors.txt

[Save]
model_dir = model
load_wordvocab_path = %(model_dir)s/wordvocab.pkl
save_wordvocab_path = %(model_dir)s/wordvocab.pkl
load_charvocab_path = %(model_dir)s/charvocab.pkl
save_charvocab_path = %(model_dir)s/charvocab.pkl
load_model_path = %(model_dir)s/att_lstm_model.pkl
save_model_path = %(model_dir)s/att_lstm_model.pkl


[Optimizer]
learning_rate = 1e-3
weight_decay = 1e-7


[Network]
train_data = 85000
test_data = 5000
dev_data = 1000
epochs = 15
# 类别数（可省去）
nb_class = 3
nb_layers = 1
# 最大序列长度（可省去）
max_len = 100
hidden_size = 128
char_hidden_size = 64
batch_size = 64
embedding_size = 300
drop_rate = 0.4
drop_embed_rate = 0.4
# 注意力监督损失值分配的权重
theta = 0.3
use_cuda = True
char_window_size = 2 3 4
